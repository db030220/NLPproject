# -*- coding: utf-8 -*-
"""NLPproject2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U7To77hxhA29YlpW_0AOoWWKdyIOE0HH
"""

!pip install --upgrade pip
1!pip install tensorflow

!pip install chatchecker --no-deps

!pip install git+https://github.com/haven-jeon/PyKoSpacing.git

# from pykospacing import spacing

# new_sent = sent.replace(" ", '') # 띄어쓰기가 없는 문장으로 만들기
# print(new_sent)

!pip install customized_konlpy

!pip install git+https://github.com/jungin500/py-hanspell

# from hanspell import spell_checker

# sent = "맞춤법이 틀린 문장이 돼었습니다."
# spelled_sent = spell_checker.check(sent)

# hanspell_sent = spelled_sent.checked
# print(hanspell_sent)

from ckonlpy.tag import Twitter
twitter = Twitter()
twitter.morphs('수아는 오늘도 과제에 찌들려 있습니다.')

# twitter.add_dictionary('', 'Noun')

twitter.morphs('수아는 오늘도 과제에 찌들려 있습니다.')

!pip install nltk textblob
!pip install textblob
!pip install pandas
!pip install requests beautifulsoup4

!pip install konlpy

import requests
from bs4 import BeautifulSoup
import nltk
from textblob import TextBlob

# nltk 데이터 다운로드
nltk.download('wordnet')
nltk.download('punkt')

# def correct_word_meaning(word):
#     # 단어의 뜻 가져오기
#     word_blob = TextBlob(word)
#     corrected_word = word_blob.correct()

#     # 단어의 뜻 출력
#     print(f"Original word: {word}")
#     print(f"Corrected word: {corrected_word}\n")
#     print(f"Word definitions:")

#     # 뜻 가져오기
#     for synset in nltk.corpus.wordnet.synsets(corrected_word):
#         print(synset.definition())

# # 링크
# url = "https://open.dict.naver.com/participation/word_list.dict#common/ranking/ko/ko/"

# # 웹페이지 가져오기
# response = requests.get(url)

# # HTML 파싱
# soup = BeautifulSoup(response.text, 'html.parser')

# # 단어 추출
# words = []
# word_elements = soup.select('.wordbook_word > a')
# for element in word_elements:
#     word = element.text.strip()
#     words.append(word)

# # 단어의 뜻 교정 및 풀이 출력
# for word in words:
#     correct_word_meaning(word)

import os
import pandas as pd

# 출력 옵션 설정
pd.set_option('display.max_rows', None)  # 모든 행 출력
pd.set_option('display.max_columns', None)  # 모든 열 출력

# 파일 경로 확인
file_path = "/content/drive/MyDrive/2023NLP/신조어데이터셋.xlsx"
print(f"File path: {file_path}")


# 엑셀 파일 읽기
df = pd.read_excel(file_path)

# 데이터 확인
print(df.head())

selected_columns = ['신조어', '신조어여부']
shinjoe_data = df[selected_columns]

# 데이터 확인
print(shinjoe_data)

# "NaN"과 "0.0"이 있는 행 삭제
shinjoe_data = shinjoe_data.dropna()  # "NaN"이 있는 행 삭제
shinjoe_data = shinjoe_data[shinjoe_data['신조어여부'] != 0.0]  # "0.0"이 있는 행 삭제

# 데이터 확인
print(shinjoe_data)

from konlpy.tag import Okt

# 형태소 분석기 초기화
okt = Okt()

# NaN 값 제거하고 문자열로 변환
df['신조어'] = df['신조어'].astype(str)

# "신조어" 열에서 단어 토큰화
tokenized_data = df['신조어'].apply(lambda x: okt.morphs(x))

# 토큰화된 데이터 확인
print(tokenized_data)

# "|" 문자 제거
tokenized_data = tokenized_data.apply(lambda x: [word.replace("|", "") for word in x])

# 결과 확인
print(tokenized_data)

# #앞으로 해야되는거 :
# 1. 첫번째 방법 나눈 단어들을 신조어 사전을 열어 입력할 수 있게 함.
# 다른 방법 나누기 직전의 단어를 신조어 사전에 넣고 나누는게 필요한 것만 나누고
# 다음에 입력할 수 있게 함.
# 2. 단어가 있을 경우임. 단어의 뜻을 사전에서 불러오기
# 3. 사전 api를 쓸수있을 지 불분명 -> 사이트를 파싱해 온다음 단어를 검색하고 이를 찾아내기

